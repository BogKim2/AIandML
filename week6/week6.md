# Week 6: Transformerì™€ Attention Mechanism

## ğŸ“š í•™ìŠµ ëª©í‘œ

ì´ë²ˆ ì£¼ì°¨ì—ì„œëŠ” í˜„ëŒ€ ë”¥ëŸ¬ë‹ì˜ í•µì‹¬ì¸ Transformer ì•„í‚¤í…ì²˜ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.

**ë°°ìš¸ ë‚´ìš©:**
1. RNNì˜ í•œê³„ì™€ Attentionì˜ ë“±ì¥ ë°°ê²½
2. Self-Attention ë©”ì»¤ë‹ˆì¦˜ì˜ ì›ë¦¬
3. Positional Encodingì˜ í•„ìš”ì„±ê³¼ êµ¬í˜„
4. ì™„ì „í•œ Transformer Block êµ¬ì¡°
5. ì‹¤ì œ Sequence Modeling ì ìš©

**ì™œ ì¤‘ìš”í•œê°€?**
- TransformerëŠ” GPT, BERT, ChatGPTì˜ ê¸°ë°˜
- ìì—°ì–´ ì²˜ë¦¬(NLP)ì˜ í˜ëª…ì„ ì¼ìœ¼í‚´
- ë³‘ë ¬ ì²˜ë¦¬ë¡œ í•™ìŠµ ì†ë„ ëŒ€í­ í–¥ìƒ
- Attentionìœ¼ë¡œ ëª¨ë¸ í•´ì„ ê°€ëŠ¥
- Computer Vision, ìŒì„± ì¸ì‹ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ë¡œ í™•ì¥

---

## ğŸ¯ ë“¤ì–´ê°€ê¸°: RNNì˜ í•œê³„

### RNNì´ ê°€ì§„ ë¬¸ì œë“¤

**1. ìˆœì°¨ ì²˜ë¦¬ (Sequential Processing)**
```
t=1 â†’ t=2 â†’ t=3 â†’ ... â†’ t=n
```
- í•œ ë²ˆì— í•œ time stepì”©ë§Œ ì²˜ë¦¬
- ë³‘ë ¬í™” ë¶ˆê°€ëŠ¥ â†’ GPU í™œìš© ì œí•œ
- ê¸´ ë¬¸ì¥ ì²˜ë¦¬ ì‹œ ë§¤ìš° ëŠë¦¼

**2. ê¸´ ê±°ë¦¬ ì˜ì¡´ì„± (Long-Range Dependencies)**
```
"The cat, which we found yesterday, is very cute."
 â†‘                                           â†‘
 ì£¼ì–´ì™€ ë™ì‚¬ ì‚¬ì´ê°€ ë©€ë©´ ì—°ê²°ì´ ì–´ë ¤ì›€
```
- ë©€ë¦¬ ë–¨ì–´ì§„ ë‹¨ì–´ ê°„ ê´€ê³„ í•™ìŠµ ì–´ë ¤ì›€
- ê·¸ë˜ë””ì–¸íŠ¸ ì†Œì‹¤/í­ë°œ ë¬¸ì œ
- LSTM/GRUë¡œ ê°œì„ í–ˆì§€ë§Œ ê·¼ë³¸ì  í•œê³„

**3. ê³ ì •ëœ Hidden State**
- ëª¨ë“  ì •ë³´ë¥¼ í•˜ë‚˜ì˜ ë²¡í„°ì— ì••ì¶•
- ì •ë³´ ë³‘ëª© í˜„ìƒ (Information Bottleneck)
- ê¸´ ë¬¸ì¥ì¼ìˆ˜ë¡ ì •ë³´ ì†ì‹¤

### Attentionì˜ ë“±ì¥

**í•µì‹¬ ì•„ì´ë””ì–´:**
> "ëª¨ë“  ìœ„ì¹˜ì—ì„œ ëª¨ë“  ìœ„ì¹˜ë¥¼ ì§ì ‘ ë³¼ ìˆ˜ ìˆë‹¤ë©´?"

**ì¥ì :**
- âœ… ë³‘ë ¬ ì²˜ë¦¬ ê°€ëŠ¥
- âœ… ê±°ë¦¬ ë¬´ê´€í•˜ê²Œ O(1) ì—°ê²°
- âœ… ì–´ë””ë¥¼ ë³´ëŠ”ì§€ í•´ì„ ê°€ëŠ¥
- âœ… ë™ì ìœ¼ë¡œ ì¤‘ìš”í•œ ì •ë³´ ì„ íƒ

---

## ğŸ”¬ Lab 1: Attentionì˜ ê¸°ì´ˆ (01_attention_basics.py)

### ëª©ì 
Attention ë©”ì»¤ë‹ˆì¦˜ì˜ ê°€ì¥ ê¸°ë³¸ì ì¸ í˜•íƒœì¸ Scaled Dot-Product Attentionì„ ì´í•´í•©ë‹ˆë‹¤.

### í”„ë¡œê·¸ë¨ ì‹¤í–‰
```bash
cd week6
python 01_attention_basics.py
```

### í•µì‹¬ ê°œë…: Query, Key, Value

**ì¼ìƒì  ë¹„ìœ :**
```
ë„ì„œê´€ì—ì„œ ì±… ì°¾ê¸°:
- Query (ì§ˆë¬¸): "ë¨¸ì‹ ëŸ¬ë‹ ì±… ì°¾ì•„ì¤˜"
- Key (ìƒ‰ì¸): ê° ì±…ì˜ ì œëª©/í‚¤ì›Œë“œ
- Value (ë‚´ìš©): ì‹¤ì œ ì±…ì˜ ë‚´ìš©

Attention = ì§ˆë¬¸ê³¼ ê°€ì¥ ê´€ë ¨ ìˆëŠ” ì±…ë“¤ì„ ê°€ì¤‘í‰ê· 
```

**ìˆ˜í•™ì  ì •ì˜:**
```
Attention(Q, K, V) = softmax(QÂ·K^T / sqrt(d_k)) Â· V

ì—¬ê¸°ì„œ:
- Q: Query í–‰ë ¬ (n_queries Ã— d_k)
- K: Key í–‰ë ¬ (n_keys Ã— d_k)
- V: Value í–‰ë ¬ (n_values Ã— d_v)
- d_k: Keyì˜ ì°¨ì›
```

### ë‹¨ê³„ë³„ ê³„ì‚°

**Step 1: ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚°**
```
Scores = Q Â· K^T
```
- ê° Queryì™€ ê° Keyì˜ ë‚´ì 
- ë‚´ì ì´ í¬ë©´ ìœ ì‚¬ë„ ë†’ìŒ

**Step 2: Scaling**
```
Scaled_Scores = Scores / sqrt(d_k)
```
- ì™œ ë‚˜ëˆ„ë‚˜? â†’ Softmaxì˜ ê·¸ë˜ë””ì–¸íŠ¸ ì•ˆì •í™”
- d_kê°€ í¬ë©´ ë‚´ì  ê°’ë„ ì»¤ì§
- ë„ˆë¬´ í° ê°’ì€ softmaxë¥¼ saturate ì‹œí‚´

**Step 3: Softmaxë¡œ í™•ë¥ í™”**
```
Attention_Weights = softmax(Scaled_Scores)
```
- ê° í–‰ì˜ í•© = 1.0 (í™•ë¥  ë¶„í¬)
- ë†’ì€ ì ìˆ˜ â†’ ë†’ì€ ê°€ì¤‘ì¹˜

**Step 4: Valueì˜ ê°€ì¤‘ í‰ê· **
```
Output = Attention_Weights Â· V
```
- ì¤‘ìš”í•œ Valueì— ë†’ì€ ê°€ì¤‘ì¹˜
- ìµœì¢… ì¶œë ¥ì€ ê´€ë ¨ ì •ë³´ì˜ ì¡°í•©

### ì£¼ìš” ê²°ê³¼

**Attention Weightsì˜ ì˜ë¯¸:**
- ê° ë‹¨ì–´ê°€ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ì— ì–¼ë§ˆë‚˜ ì£¼ëª©í•˜ëŠ”ì§€
- Heatmapìœ¼ë¡œ ì‹œê°í™” ê°€ëŠ¥
- ëª¨ë¸ì´ "ì–´ë””ë¥¼ ë³´ëŠ”ì§€" ì•Œ ìˆ˜ ìˆìŒ

**Scalingì˜ íš¨ê³¼:**
- Scaling ì—†ìœ¼ë©´: ê·¹ë‹¨ì  í™•ë¥  (0.9, 0.05, 0.05)
- Scaling ìˆìœ¼ë©´: ë¶€ë“œëŸ¬ìš´ ë¶„í¬ (0.5, 0.3, 0.2)
- í•™ìŠµ ì•ˆì •ì„± í–¥ìƒ

### ì¶œë ¥ íŒŒì¼
- `outputs/01_attention_weights.png`: Attention ê°€ì¤‘ì¹˜ í–‰ë ¬
- `outputs/01_scaling_effect.png`: Scaling íš¨ê³¼ ë¹„êµ
- `outputs/01_longer_sequence.png`: ê¸´ ë¬¸ì¥ì—ì„œì˜ Attention

---

## ğŸ”¬ Lab 2: Self-Attentionê³¼ Multi-Head (02_self_attention.py)

### ëª©ì 
Self-Attentionì˜ ì›ë¦¬ì™€ Multi-Head Attentionì´ ì™œ í•„ìš”í•œì§€ ì´í•´í•©ë‹ˆë‹¤.

### Self-Attentionì´ë€?

**ì¼ë°˜ Attention:**
```
Encoder â†’ Decoder ê°„ attention
Query: Decoderì˜ í˜„ì¬ ìƒíƒœ
Key, Value: Encoderì˜ ëª¨ë“  ì¶œë ¥
```

**Self-Attention:**
```
Query, Key, Value ëª¨ë‘ ê°™ì€ ì…ë ¥ì—ì„œ!
ìê¸° ìì‹  ë‚´ë¶€ì˜ ê´€ê³„ë¥¼ í•™ìŠµ
```

**ì˜ˆì‹œ:**
```
ë¬¸ì¥: "The cat sat on the mat"

"cat"ì˜ Self-Attention:
- "The"ì™€ì˜ ê´€ê³„: 0.15 (ê´€ì‚¬)
- "cat"ì™€ì˜ ê´€ê³„: 0.30 (ìê¸° ìì‹ )
- "sat"ì™€ì˜ ê´€ê³„: 0.35 (ë™ì‚¬ - ì¤‘ìš”!)
- "on"ì™€ì˜ ê´€ê³„: 0.05
- "the"ì™€ì˜ ê´€ê³„: 0.05
- "mat"ì™€ì˜ ê´€ê³„: 0.10 (ëª©ì ì–´)

â†’ "cat"ì€ ì£¼ë¡œ "sat"ì— ì£¼ëª©!
```

### Multi-Head Attention

**ì™œ ì—¬ëŸ¬ ê°œì˜ Head?**

**ë‹¨ì¼ Attentionì˜ í•œê³„:**
- í•œ ë²ˆì— í•˜ë‚˜ì˜ ê´€ì ë§Œ í•™ìŠµ
- "cat sat" (ì£¼ì–´-ë™ì‚¬) ê´€ê³„ í•™ìŠµ ì¤‘ì´ë©´
- "cat on mat" (ìœ„ì¹˜ ê´€ê³„)ëŠ” ë†“ì¹  ìˆ˜ ìˆìŒ

**Multi-Headì˜ ì¥ì :**
```
Head 1: ì£¼ì–´-ë™ì‚¬ ê´€ê³„
Head 2: ìˆ˜ì‹ì–´ ê´€ê³„
Head 3: ìœ„ì¹˜ ê´€ê³„
Head 4: ì˜ë¯¸ì  ìœ ì‚¬ì„±

â†’ ë³‘ë ¬ë¡œ ë‹¤ì–‘í•œ íŒ¨í„´ í•™ìŠµ!
```

**êµ¬í˜„:**
```
ê° Headë§ˆë‹¤ ë…ë¦½ì ì¸ Q, K, V ë³€í™˜
Head_i = Attention(Q_i, K_i, V_i)

MultiHead = Concat(Head_1, ..., Head_h) Â· W_O
```

### RNN vs Self-Attention ë¹„êµ

**ê³„ì‚° ë³µì¡ë„:**
```
RNN:
- ì‹œê°„ ë³µì¡ë„: O(n) steps (sequential)
- í•œ ìŠ¤í…ë‹¹: O(dÂ²) ì—°ì‚°
- ë³‘ë ¬í™”: ë¶ˆê°€ëŠ¥

Self-Attention:
- ì‹œê°„ ë³µì¡ë„: O(1) steps (parallel)
- ì´ ì—°ì‚°: O(nÂ²Â·d)
- ë³‘ë ¬í™”: ì™„ì „ ê°€ëŠ¥
```

**Path Length (ì •ë³´ ì „ë‹¬ ê²½ë¡œ):**
```
RNN: ìœ„ì¹˜ i â†’ jê¹Œì§€ |i-j| ìŠ¤í…
Self-Attention: ëª¨ë“  ìœ„ì¹˜ ê°„ 1 ìŠ¤í…!

"I love you very very much"
 â†‘                        â†‘
RNN: 5 ìŠ¤í… ê±°ì³ì•¼ ì—°ê²°
Attention: ì§ì ‘ ì—°ê²°!
```

**Trade-off:**
```
n < d (ì§§ì€ ì‹œí€€ìŠ¤, í° ì°¨ì›):
  â†’ Attentionì´ íš¨ìœ¨ì 

n >> d (ë§¤ìš° ê¸´ ì‹œí€€ìŠ¤):
  â†’ RNNì´ ë©”ëª¨ë¦¬ íš¨ìœ¨ì 
  â†’ Sparse Attention ë“± ê°œì„  ë°©ë²• í•„ìš”
```

### ì£¼ìš” ê´€ì°°

**Headì˜ ì „ë¬¸í™”:**
- ê° Headê°€ ë‹¤ë¥¸ íŒ¨í„´ í•™ìŠµ
- ì–´ë–¤ HeadëŠ” ì¸ì ‘ ë‹¨ì–´ ì§‘ì¤‘
- ì–´ë–¤ HeadëŠ” ë¨¼ ê±°ë¦¬ ì˜ì¡´ì„± í¬ì°©

**Attention Diversity:**
- Head ê°„ ë¶„ì‚°ì´ í´ìˆ˜ë¡ ì¢‹ìŒ
- ë‹¤ì–‘í•œ ì •ë³´ ì¶”ì¶œ

### ì¶œë ¥ íŒŒì¼
- `outputs/02_self_attention_components.png`: Self-Attention êµ¬ì„± ìš”ì†Œ
- `outputs/02_multi_head_attention.png`: Multi-Head íŒ¨í„´ ë¹„êµ
- `outputs/02_rnn_vs_attention.png`: RNNê³¼ ë³µì¡ë„ ë¹„êµ

---

## ğŸ”¬ Lab 3: Positional Encoding (03_positional_encoding.py)

### ëª©ì 
Self-Attentionì€ ìˆœì„œë¥¼ ëª¨ë¦„ - Positional Encodingìœ¼ë¡œ ìœ„ì¹˜ ì •ë³´ ì¶”ê°€

### ë¬¸ì œ: Permutation Invariance

**Self-Attentionì˜ ë§¹ì :**
```
Input 1: "I love you"
Input 2: "You love I"

Self-Attentionë§Œ ì‚¬ìš©í•˜ë©´:
â†’ ê°™ì€ ì¶œë ¥! (ìˆœì„œ ë¬´ì‹œ)
```

**ì¦ëª…:**
```
Attention(Q, K, V)ëŠ” ì§‘í•© ì—°ì‚°
ë‹¨ì–´ ìˆœì„œë¥¼ ë°”ê¿”ë„ ê²°ê³¼ ë™ì¼
â†’ ìœ„ì¹˜ ì •ë³´ê°€ ì—†ìŒ!
```

### í•´ê²°: Positional Encoding

**ê¸°ë³¸ ì•„ì´ë””ì–´:**
```
Word_Embedding + Positional_Encoding = Final_Input

ì˜ˆ:
"cat" ì„ë² ë”©: [0.2, 0.5, -0.1, ...]
Position 3 ì¸ì½”ë”©: [0.1, -0.2, 0.3, ...]
â†’ Final: [0.3, 0.3, 0.2, ...]
```

### Sinusoidal Positional Encoding

**ìˆ˜ì‹ (Vaswani et al., 2017):**
```
PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))

ì—¬ê¸°ì„œ:
- pos: ìœ„ì¹˜ (0, 1, 2, ...)
- i: ì°¨ì› ì¸ë±ìŠ¤
- d_model: ì„ë² ë”© ì°¨ì›
```

**ì§ê´€ì  ì´í•´:**
```
ì§ìˆ˜ ì°¨ì›: sin í•¨ìˆ˜
í™€ìˆ˜ ì°¨ì›: cos í•¨ìˆ˜

ë‚®ì€ ì°¨ì› (iê°€ ì‘ìŒ):
  â†’ ë†’ì€ ì£¼íŒŒìˆ˜ â†’ ë¹ ë¥´ê²Œ ë³€í•¨
  â†’ ì„¸ë°€í•œ ìœ„ì¹˜ êµ¬ë¶„

ë†’ì€ ì°¨ì› (iê°€ í¼):
  â†’ ë‚®ì€ ì£¼íŒŒìˆ˜ â†’ ì²œì²œíˆ ë³€í•¨
  â†’ í° ë²”ìœ„ì˜ íŒ¨í„´
```

**ì™œ Sine/Cosine?**

1. **ì£¼ê¸° í•¨ìˆ˜** â†’ ì—°ì†ì ì´ê³  ë¶€ë“œëŸ¬ì›€
2. **ë‹¤ì–‘í•œ ì£¼íŒŒìˆ˜** â†’ ì—¬ëŸ¬ ìŠ¤ì¼€ì¼ í‘œí˜„
3. **ìƒëŒ€ ìœ„ì¹˜ í‘œí˜„:**
   ```
   PE(pos+k)ëŠ” PE(pos)ì˜ ì„ í˜• í•¨ìˆ˜
   â†’ ëª¨ë¸ì´ ìƒëŒ€ ê±°ë¦¬ í•™ìŠµ ê°€ëŠ¥
   ```
4. **í•™ìŠµ ë¶ˆí•„ìš”** â†’ íŒŒë¼ë¯¸í„° 0ê°œ
5. **ì™¸ì‚½ ê°€ëŠ¥** â†’ í•™ìŠµ ì‹œë³´ë‹¤ ê¸´ ì‹œí€€ìŠ¤ë„ OK

### ë‹¤ë¥¸ ë°©ë²•ë“¤ê³¼ ë¹„êµ

**1. Learned Positional Embeddings:**
```python
pos_embedding = nn.Embedding(max_length, d_model)
```
- ì¥ì : ìœ ì—°í•¨, í•™ìŠµ ê°€ëŠ¥
- ë‹¨ì : max_lengthë³´ë‹¤ ê¸´ ì‹œí€€ìŠ¤ ë¶ˆê°€
- ì‚¬ìš©: BERT, GPT ì´ˆê¸° ë²„ì „

**2. Linear Encoding:**
```
PE(pos) = pos / max_length
```
- ê°„ë‹¨í•˜ì§€ë§Œ ì •ë³´ëŸ‰ ë¶€ì¡±

**3. Relative Positional Encoding:**
- ì ˆëŒ€ ìœ„ì¹˜ ëŒ€ì‹  ìƒëŒ€ ê±°ë¦¬
- Transformer-XL, T5ì—ì„œ ì‚¬ìš©

### íš¨ê³¼ ë¶„ì„

**ìœ„ì¹˜ ì •ë³´ ì—†ìœ¼ë©´:**
```
"I love you" = "you love I"
ë¬¸ë²•ì ìœ¼ë¡œ ì™„ì „íˆ ë‹¤ë¥¸ ì˜ë¯¸ì¸ë° êµ¬ë¶„ ëª»í•¨
```

**ìœ„ì¹˜ ì •ë³´ ì¶”ê°€ í›„:**
```
ê° ë‹¨ì–´ê°€ ìœ„ì¹˜ë³„ ê³ ìœ í•œ í‘œí˜„
ìˆœì„œê°€ ë°”ë€Œë©´ ë‹¤ë¥¸ ë²¡í„°
â†’ ë¬¸ì¥ êµ¬ì¡° í•™ìŠµ ê°€ëŠ¥
```

**Similarity Pattern:**
```
ì¸ì ‘í•œ ìœ„ì¹˜: ë†’ì€ ìœ ì‚¬ë„
ë¨¼ ìœ„ì¹˜: ë‚®ì€ ìœ ì‚¬ë„
â†’ ëª¨ë¸ì´ ê±°ë¦¬ ê°ê° ê°€ì§
```

### ì¶œë ¥ íŒŒì¼
- `outputs/03_positional_encoding_sinusoidal.png`: Sinusoidal ì¸ì½”ë”© íŒ¨í„´
- `outputs/03_encoding_comparison.png`: ë‹¤ì–‘í•œ ì¸ì½”ë”© ë°©ë²• ë¹„êµ
- `outputs/03_relative_position.png`: ìœ„ì¹˜ ê°„ ìœ ì‚¬ë„
- `outputs/03_position_effect.png`: Attentionì— ë¯¸ì¹˜ëŠ” ì˜í–¥

---

## ğŸ”¬ Lab 4: ì™„ì „í•œ Transformer Block (04_transformer_block.py)

### ëª©ì 
ëª¨ë“  ìš”ì†Œë¥¼ ê²°í•©í•œ ì™„ì „í•œ Transformer Encoder Blockì„ êµ¬í˜„í•˜ê³  ì´í•´í•©ë‹ˆë‹¤.

### Transformer Block êµ¬ì¡°

```
Input
  â†“
[Multi-Head Self-Attention]
  â†“
[Add & Normalize] â† Residual Connection
  â†“
[Feed-Forward Network]
  â†“
[Add & Normalize] â† Residual Connection
  â†“
Output
```

### ê° ì»´í¬ë„ŒíŠ¸ ìƒì„¸

**1. Multi-Head Self-Attention**
```
ì´ë¯¸ ë°°ìš´ ë‚´ìš©:
- ì—¬ëŸ¬ Headë¡œ ë‹¤ì–‘í•œ íŒ¨í„´ í•™ìŠµ
- ë³‘ë ¬ ì²˜ë¦¬
- ê´€ê³„ í¬ì°©
```

**2. Residual Connection (Add)**
```
Output = X + Sublayer(X)

ì™œ í•„ìš”í•œê°€?
```

**ê·¸ë˜ë””ì–¸íŠ¸ ì†Œì‹¤ ë¬¸ì œ:**
```
ê¹Šì€ ë„¤íŠ¸ì›Œí¬:
Layer 1 â†’ Layer 2 â†’ ... â†’ Layer N

Backpropagation ì‹œ:
âˆ‚Loss/âˆ‚Layer1 = âˆ‚Loss/âˆ‚LayerN Ã— âˆ‚LayerN/âˆ‚Layer(N-1) Ã— ...

ê° ê³±ì…ˆë§ˆë‹¤ ê°’ì´ ì‘ì•„ì§€ë©´
â†’ ê·¸ë˜ë””ì–¸íŠ¸ ì†Œì‹¤ (Vanishing Gradient)
```

**Residualì˜ í•´ê²°ì±…:**
```
y = x + F(x)

âˆ‚y/âˆ‚x = 1 + âˆ‚F(x)/âˆ‚x

í•­ìƒ ìµœì†Œ 1 ì´ìƒ!
â†’ ê·¸ë˜ë””ì–¸íŠ¸ê°€ ì§ì ‘ íë¥¼ ìˆ˜ ìˆëŠ” ê²½ë¡œ (Highway)
â†’ ë§¤ìš° ê¹Šì€ ë„¤íŠ¸ì›Œí¬ í•™ìŠµ ê°€ëŠ¥
```

**ì •ë³´ ë³´ì¡´:**
```
ì…ë ¥ ì •ë³´ê°€ ì¶œë ¥ê¹Œì§€ ì§ì ‘ ì „ë‹¬
â†’ Identity mapping
â†’ í•™ìŠµ ì´ˆê¸°ì—ë„ ì•ˆì •ì 
```

**3. Layer Normalization**
```
LN(x) = Î³ Â· (x - Î¼) / Ïƒ + Î²

ì—¬ê¸°ì„œ:
- Î¼: í‰ê·  (across features)
- Ïƒ: í‘œì¤€í¸ì°¨ (across features)
- Î³, Î²: í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°
```

**Batch Norm vs Layer Norm:**
```
Batch Normalization:
- ë°°ì¹˜ ë‚´ ìƒ˜í”Œë“¤ ê°„ ì •ê·œí™”
- ë°°ì¹˜ í¬ê¸°ì— ì˜ì¡´
- RNNì— ë¶€ì í•©

Layer Normalization:
- ê° ìƒ˜í”Œì˜ feature ê°„ ì •ê·œí™”
- ë°°ì¹˜ ë…ë¦½ì 
- RNN, Transformerì— ì í•©
```

**íš¨ê³¼:**
```
1. Internal Covariate Shift ê°ì†Œ
2. í•™ìŠµ ì•ˆì •í™”
3. ë” í° learning rate ì‚¬ìš© ê°€ëŠ¥
4. ë¹ ë¥¸ ìˆ˜ë ´
```

**4. Feed-Forward Network (FFN)**
```
FFN(x) = GELU(xÂ·Wâ‚ + bâ‚)Â·Wâ‚‚ + bâ‚‚

êµ¬ì¡°:
d_model â†’ d_ff â†’ d_model
ë³´í†µ d_ff = 4 Ã— d_model

ì˜ˆ: 512 â†’ 2048 â†’ 512
```

**ì™œ í•„ìš”í•œê°€?**

**ë¹„ì„ í˜•ì„± ì¶”ê°€:**
```
Attentionì€ ì„ í˜• ë³€í™˜ë“¤ì˜ ì¡°í•©
â†’ í‘œí˜„ë ¥ í•œê³„

FFNì˜ ë¹„ì„ í˜• í™œì„±í™”:
â†’ ë³µì¡í•œ íŒ¨í„´ í•™ìŠµ ê°€ëŠ¥
```

**Position-wise:**
```
ê° ìœ„ì¹˜ì— ë…ë¦½ì ìœ¼ë¡œ ì ìš©
ê°™ì€ ë³€í™˜, í•˜ì§€ë§Œ ë³‘ë ¬ ì²˜ë¦¬
```

**Capacity ì¦ê°€:**
```
d_ffê°€ í¬ë©´ (4x):
â†’ ë” ë§ì€ íŒ¨í„´ í•™ìŠµ
â†’ ëª¨ë¸ ìš©ëŸ‰ ì¦ê°€
```

**GELU vs ReLU:**
```
ReLU: max(0, x)
  - ê°„ë‹¨, ë¹ ë¦„
  - ìŒìˆ˜ ì •ë³´ ì™„ì „ ì†ì‹¤

GELU: xÂ·Î¦(x) (Gaussian Error Linear Unit)
  - ë¶€ë“œëŸ¬ìš´ ê³¡ì„ 
  - ìŒìˆ˜ë„ ì‘ì€ ê°’ìœ¼ë¡œ ì „ë‹¬
  - Transformerì—ì„œ ì„±ëŠ¥ ìš°ìˆ˜
```

### ë°ì´í„° íë¦„ ë¶„ì„

**ê° ë‹¨ê³„ì—ì„œ ë³€í™”:**
```
1. Input: ì›ë³¸ ì„ë² ë”©
2. Attention: ë¬¸ë§¥ ì •ë³´ ì¶”ê°€
3. Residual 1: ì›ë³¸ + ë¬¸ë§¥
4. Norm 1: ì •ê·œí™”
5. FFN: ë¹„ì„ í˜• ë³€í™˜
6. Residual 2: ì •ë³´ ë³´ì¡´
7. Norm 2: ìµœì¢… ì •ê·œí™”
```

**í†µê³„ì  ë³€í™”:**
```
í‰ê·  (Î¼):
- LayerNorm í›„ â‰ˆ 0

ë¶„ì‚° (ÏƒÂ²):
- LayerNorm í›„ â‰ˆ 1

â†’ í•™ìŠµ ì•ˆì •ì„± í–¥ìƒ
```

### Pre-Norm vs Post-Norm

**Post-Norm (Original Transformer):**
```
X â†’ Sublayer â†’ Add â†’ Norm
```

**Pre-Norm (í˜„ëŒ€ì ):**
```
X â†’ Norm â†’ Sublayer â†’ Add
```

**Pre-Norm ì¥ì :**
```
- í•™ìŠµ ë” ì•ˆì •ì 
- í° ëª¨ë¸ì—ì„œ ìœ ë¦¬
- Warm-up ëœ í•„ìš”
- GPT-3, T5 ë“±ì—ì„œ ì‚¬ìš©
```

### ì¶œë ¥ íŒŒì¼
- `outputs/04_transformer_dataflow.png`: ë°ì´í„° íë¦„ ë‹¨ê³„ë³„ ì‹œê°í™”
- `outputs/04_attention_patterns.png`: Multi-Head Attention íŒ¨í„´
- `outputs/04_residual_effect.png`: Residual Connection íš¨ê³¼

---

## ğŸ”¬ Lab 5: ì‹¤ì „ Sequence Modeling (05_sequence_modeling.py)

### ëª©ì 
Transformerë¥¼ ì‹¤ì œ ì‹œí€€ìŠ¤ ì˜ˆì¸¡ ë¬¸ì œì— ì ìš©í•˜ê³  RNNê³¼ ë¹„êµí•©ë‹ˆë‹¤.

### ì‹¤í—˜ ì„¤ì •

**Task: ì‹œê³„ì—´ ì˜ˆì¸¡**
```
Input: [yâ‚€, yâ‚, ..., yâ‚â‚‰]  (20 time steps)
Output: yâ‚‚â‚€                 (ë‹¤ìŒ ê°’ ì˜ˆì¸¡)

Data: ì‚¬ì¸íŒŒ
y(t) = AÂ·sin(fÂ·t + Ï†)
- A: ì§„í­ (random)
- f: ì£¼íŒŒìˆ˜ (random)
- Ï†: ìœ„ìƒ (random)
```

### ëª¨ë¸ êµ¬ì„±

**Transformer:**
```
1. Input Projection: 1D â†’ 32D
2. Positional Encoding ì¶”ê°€
3. Multi-Head Attention (4 heads)
4. Feed-Forward (32 â†’ 128 â†’ 32)
5. Pooling (ë§ˆì§€ë§‰ ìœ„ì¹˜)
6. Output Projection: 32D â†’ 1D
```

**RNN (ë¹„êµêµ°):**
```
1. Hidden state: 64D
2. Sequential processing
3. Output: ìµœì¢… hidden state â†’ 1D
```

### Attention ë¶„ì„

**ë¬´ì—‡ì„ í•™ìŠµí–ˆë‚˜?**

**ìœ„ì¹˜ë³„ ì¤‘ìš”ë„:**
```
ë§ˆì§€ë§‰ ìœ„ì¹˜ì—ì„œ ê° ê³¼ê±° ìœ„ì¹˜ë¡œì˜ attention:

ê°€ê¹Œìš´ ê³¼ê±° (t-1, t-2, t-3):
  â†’ ë†’ì€ attention (0.3~0.5)
  â†’ ìµœê·¼ ì¶”ì„¸ ì¤‘ìš”

ì¤‘ê°„ ê³¼ê±° (t-10):
  â†’ ì¤‘ê°„ attention (0.1~0.2)
  â†’ ì£¼ê¸° íŒŒì•…

ë¨¼ ê³¼ê±° (t-20):
  â†’ ë‚®ì€ attention (0.05)
  â†’ ëœ ì¤‘ìš”
```

**Multi-Head ì „ë¬¸í™”:**
```
Head 1: ì§ì „ ê°’ì— ì§‘ì¤‘ (local trend)
Head 2: ì£¼ê¸°ì  íŒ¨í„´ í¬ì°© (periodicity)
Head 3: ì „ì²´ ë²”ìœ„ ê³ ë ¤ (global context)
Head 4: íŠ¹ì • ìœ„ì¹˜ ì¡°í•© (specific patterns)
```

### ì„±ëŠ¥ ë¹„êµ

**ì •ëŸ‰ì  ê²°ê³¼:**
```
MSE Loss (ì˜ˆì‹œ):
- Transformer: 0.0234 Â± 0.0156
- RNN: 0.0289 Â± 0.0198

â†’ Transformerê°€ ì•½ê°„ ìš°ìˆ˜
â†’ ë¶„ì‚°ë„ ë” ì‘ìŒ (ì•ˆì •ì )
```

**ì™œ Transformerê°€ ì¢‹ì€ê°€?**

1. **ë³‘ë ¬ ì²˜ë¦¬:**
   ```
   ì „ì²´ ì‹œí€€ìŠ¤ë¥¼ í•œ ë²ˆì— ì²˜ë¦¬
   â†’ ë” í’ë¶€í•œ ë¬¸ë§¥ ì •ë³´
   ```

2. **ì§ì ‘ ì—°ê²°:**
   ```
   t=0ê³¼ t=20ì´ ì§ì ‘ ì—°ê²°
   â†’ RNNì€ 20 ìŠ¤í… ê±°ì³ì•¼ í•¨
   â†’ ì¥ê±°ë¦¬ ì˜ì¡´ì„± í•™ìŠµ ìœ ë¦¬
   ```

3. **í•´ì„ ê°€ëŠ¥ì„±:**
   ```
   Attention weights ì‹œê°í™”
   â†’ ì–´ëŠ ì‹œì ì´ ì¤‘ìš”í•œì§€ ì•Œ ìˆ˜ ìˆìŒ
   â†’ RNNì˜ hidden stateëŠ” ë¶ˆíˆ¬ëª…
   ```

### ì‹¤ì „ ì ìš© ì‹œ ê³ ë ¤ì‚¬í•­

**ì–¸ì œ Transformer?**
```
âœ… ë¬¸ë§¥ì´ ì¤‘ìš”í•œ ì‘ì—… (ë²ˆì—­, ìš”ì•½)
âœ… ë³‘ë ¬ ì²˜ë¦¬ ê°€ëŠ¥í•œ í™˜ê²½ (GPU)
âœ… í•´ì„ì´ í•„ìš”í•œ ê²½ìš°
âœ… ì¤‘ê°„ ê¸¸ì´ ì‹œí€€ìŠ¤ (ìˆ˜ë°±~ìˆ˜ì²œ)
```

**ì–¸ì œ RNN?**
```
âœ… ë§¤ìš° ê¸´ ì‹œí€€ìŠ¤ (ë©”ëª¨ë¦¬ ì œì•½)
âœ… ì˜¨ë¼ì¸/ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
âœ… ìˆœì°¨ì  ì˜ì¡´ì„±ì´ ê°•í•œ ê²½ìš°
âœ… ì‘ì€ ëª¨ë¸ í•„ìš” (ëª¨ë°”ì¼)
```

**Hybrid Approaches:**
```
1. Conformer (Speech):
   Convolution + Transformer

2. Longformer:
   Local + Global Attention

3. Linformer:
   Linear complexity Attention
```

### ì¶œë ¥ íŒŒì¼
- `outputs/05_sample_sequences.png`: ì…ë ¥ ë°ì´í„° ì˜ˆì‹œ
- `outputs/05_attention_patterns.png`: Attention íŒ¨í„´ ë¶„ì„
- `outputs/05_performance_comparison.png`: Transformer vs RNN ì„±ëŠ¥
- `outputs/05_multihead_analysis.png`: Multi-Head ìƒì„¸ ë¶„ì„
- `outputs/05_position_importance.png`: ìœ„ì¹˜ë³„ ì¤‘ìš”ë„

---

## ğŸ“Š ì£¼ìš” ê²°ê³¼ ìš”ì•½

### Transformerì˜ í•µì‹¬ ìš”ì†Œ

**1. Attention Mechanism:**
```
- Query, Key, Valueë¡œ ê´€ë ¨ì„± ê³„ì‚°
- Softmaxë¡œ ê°€ì¤‘ì¹˜ ê²°ì •
- ë™ì ì´ê³  í•´ì„ ê°€ëŠ¥
```

**2. Self-Attention:**
```
- ì…ë ¥ ë‚´ë¶€ì˜ ê´€ê³„ í•™ìŠµ
- ëª¨ë“  ìœ„ì¹˜ ê°„ O(1) ì—°ê²°
- ë³‘ë ¬ ì²˜ë¦¬ ê°€ëŠ¥
```

**3. Multi-Head:**
```
- ë‹¤ì–‘í•œ íŒ¨í„´ ë™ì‹œ í•™ìŠµ
- Headë³„ ì „ë¬¸í™”
- í‘œí˜„ë ¥ ì¦ê°€
```

**4. Positional Encoding:**
```
- ìˆœì„œ ì •ë³´ ì¶”ê°€
- Sinusoidal: í•™ìŠµ ë¶ˆí•„ìš”
- ìƒëŒ€ ìœ„ì¹˜ í‘œí˜„ ê°€ëŠ¥
```

**5. Residual + LayerNorm:**
```
- ê¹Šì€ ë„¤íŠ¸ì›Œí¬ í•™ìŠµ ê°€ëŠ¥
- ê·¸ë˜ë””ì–¸íŠ¸ íë¦„ ê°œì„ 
- í•™ìŠµ ì•ˆì •í™”
```

**6. Feed-Forward:**
```
- ë¹„ì„ í˜•ì„± ì¶”ê°€
- ìš©ëŸ‰ ì¦ê°€ (4x expansion)
- Position-wise ì ìš©
```

### ë³µì¡ë„ ë¶„ì„

**Time Complexity:**
```
Self-Attention: O(nÂ²Â·d)
  - n: ì‹œí€€ìŠ¤ ê¸¸ì´
  - d: ì„ë² ë”© ì°¨ì›

RNN: O(nÂ·dÂ²)
  - ìˆœì°¨ ì²˜ë¦¬ í•„ìš”

n < d: Attentionì´ ìœ ë¦¬
n > d: RNNì´ ìœ ë¦¬ (ë³´í†µ dê°€ ë” í¼)
```

**Space Complexity:**
```
Attention: O(nÂ²) (attention matrix)
RNN: O(1) (hidden stateë§Œ)

â†’ ë§¤ìš° ê¸´ ì‹œí€€ìŠ¤: ë©”ëª¨ë¦¬ ë¶€ë‹´
```

**Path Length:**
```
Attention: O(1) (ì§ì ‘ ì—°ê²°)
RNN: O(n) (ìˆœì°¨ ì „ë‹¬)

â†’ Attentionì´ ì¥ê±°ë¦¬ ì˜ì¡´ì„± í•™ìŠµ ìœ ë¦¬
```

### ì‹¤ì „ ì„±ëŠ¥

**NLP Tasks:**
```
ê¸°ê³„ ë²ˆì—­: BLEU ì ìˆ˜ í–¥ìƒ
ê°ì„± ë¶„ì„: ì •í™•ë„ í–¥ìƒ
ì§ˆì˜ ì‘ë‹µ: F1 score í–¥ìƒ

â†’ ëŒ€ë¶€ë¶„ì˜ NLPì—ì„œ SOTA
```

**ì¥ì :**
```
1. ë³‘ë ¬ ì²˜ë¦¬ â†’ ë¹ ë¥¸ í•™ìŠµ
2. ì¥ê±°ë¦¬ ì˜ì¡´ì„± í¬ì°©
3. í•´ì„ ê°€ëŠ¥ (attention weights)
4. ì „ì´ í•™ìŠµ (Pre-training)
```

**ë‹¨ì :**
```
1. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í¼ (O(nÂ²))
2. ë§¤ìš° ê¸´ ì‹œí€€ìŠ¤ ì–´ë ¤ì›€
3. íŒŒë¼ë¯¸í„° ìˆ˜ ë§ìŒ
4. ì‘ì€ ë°ì´í„°ì…‹ì—ì„œ overfitting
```

---

## ğŸ’¡ ì‹¤ì „ íŒ

### í”„ë¡œê·¸ë¨ ì‹¤í–‰ ìˆœì„œ

1. **01_attention_basics.py** (~1ë¶„)
   - Attention ë©”ì»¤ë‹ˆì¦˜ ê¸°ì´ˆ
   - Query-Key-Value ì´í•´
   - Scaling íš¨ê³¼

2. **02_self_attention.py** (~2ë¶„)
   - Self-Attention
   - Multi-Head
   - RNN ë¹„êµ

3. **03_positional_encoding.py** (~1ë¶„)
   - ìœ„ì¹˜ ì¸ì½”ë”© í•„ìš”ì„±
   - Sinusoidal íŒ¨í„´
   - íš¨ê³¼ ë¶„ì„

4. **04_transformer_block.py** (~2ë¶„)
   - ì™„ì „í•œ ë¸”ë¡ êµ¬ì¡°
   - Residual íš¨ê³¼
   - LayerNorm ì—­í• 

5. **05_sequence_modeling.py** (~2ë¶„)
   - ì‹¤ì œ ì ìš©
   - ì„±ëŠ¥ ë¹„êµ
   - Attention í•´ì„

### í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì •

**Model Dimension (d_model):**
```
ì‘ê²Œ (32-64): ë¹ ë¥´ì§€ë§Œ ìš©ëŸ‰ ë¶€ì¡±
ì¤‘ê°„ (128-256): ì¼ë°˜ì  ì‘ì—…
í¬ê²Œ (512-1024): ë³µì¡í•œ ì‘ì—…, í° ë°ì´í„°

ê¶Œì¥: 64ë°°ìˆ˜ (GPU ìµœì í™”)
```

**Number of Heads:**
```
ë³´í†µ: 4, 8, 16
d_model % n_heads == 0 í•„ìˆ˜

ë§ì„ìˆ˜ë¡: ë‹¤ì–‘í•œ íŒ¨í„´, í•˜ì§€ë§Œ headë‹¹ ì°¨ì› ê°ì†Œ
```

**FFN Dimension:**
```
ì¼ë°˜ì : d_ff = 4 Ã— d_model
í¬ê²Œ: 6x, 8x (ìš©ëŸ‰ ì¦ê°€)
```

**Layer ê°œìˆ˜:**
```
ì‘ì€ ì‘ì—…: 2-4 layers
ì¤‘ê°„ ì‘ì—…: 6-12 layers (BERT Base)
í° ì‘ì—…: 24+ layers (GPT-3)
```

### í•™ìŠµ íŒ

**1. Warm-up Learning Rate:**
```
ì²˜ìŒì—ëŠ” ì‘ì€ lrë¡œ ì‹œì‘
ì ì§„ì ìœ¼ë¡œ ì¦ê°€
ì´í›„ ê°ì†Œ

â†’ ì´ˆê¸° ë¶ˆì•ˆì •ì„± ë°©ì§€
```

**2. Label Smoothing:**
```
Hard target: [0, 0, 1, 0]
Soft target: [0.025, 0.025, 0.9, 0.025]

â†’ Overconfidence ë°©ì§€
```

**3. Dropout:**
```
Attention weightsì— dropout
FFNì— dropout

â†’ Overfitting ë°©ì§€
```

### ë””ë²„ê¹…

**ìì£¼ í•˜ëŠ” ì‹¤ìˆ˜:**

1. **Positional Encoding ë¹ ëœ¨ë¦¼:**
   ```
   ì¦ìƒ: ìˆœì„œ ë¬´ì‹œ
   í•´ê²°: ì„ë² ë”©ì— pos_enc ì¶”ê°€ í™•ì¸
   ```

2. **Dimension ë¶ˆì¼ì¹˜:**
   ```
   ì¦ìƒ: Shape error
   í•´ê²°: d_model % n_heads == 0 í™•ì¸
   ```

3. **Scaling ëˆ„ë½:**
   ```
   ì¦ìƒ: í•™ìŠµ ë¶ˆì•ˆì •
   í•´ê²°: scores / sqrt(d_k) í™•ì¸
   ```

4. **Softmax axis ì˜ëª»:**
   ```
   ì¦ìƒ: ì´ìƒí•œ attention
   í•´ê²°: axis=-1 (ë§ˆì§€ë§‰ ì°¨ì›)
   ```

---

## ğŸ“– ë” ê³µë¶€í•˜ë ¤ë©´

### í•„ìˆ˜ ë…¼ë¬¸

1. **"Attention Is All You Need" (2017)**
   - Vaswani et al.
   - ì›ì¡° Transformer ë…¼ë¬¸
   - ë°˜ë“œì‹œ ì½ì–´ì•¼ í•¨!

2. **"BERT" (2018)**
   - Bidirectional Encoder
   - Pre-training + Fine-tuning
   - NLPì˜ í˜ëª…

3. **"GPT-3" (2020)**
   - 175B íŒŒë¼ë¯¸í„°
   - Few-shot learning
   - ê±°ëŒ€ ëª¨ë¸ì˜ ê°€ëŠ¥ì„±

### ë°œì „ëœ ì•„í‚¤í…ì²˜

**Efficient Transformers:**
- Linformer: Linear complexity
- Reformer: LSH attention
- Longformer: Local + Global
- Performer: FAVOR+ mechanism

**Vision Transformers:**
- ViT: Image classification
- DETR: Object detection
- Swin: Hierarchical structure

**Multi-modal:**
- CLIP: Image + Text
- DALL-E: Text â†’ Image
- Flamingo: Vision-Language

### ì‹¤ìŠµ ìë£Œ

**ì½”ë“œ:**
- [Harvard NLP's Annotated Transformer](http://nlp.seas.harvard.edu/annotated-transformer/)
- [HuggingFace Transformers](https://github.com/huggingface/transformers)
- [PyTorch Tutorial](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)

**ê°•ì˜:**
- Stanford CS224N (NLP with Deep Learning)
- CMU 11-747 (Neural Networks for NLP)
- Fast.ai NLP Course

**ì±…:**
- "Natural Language Processing with Transformers" (HuggingFace)
- "Speech and Language Processing" (Jurafsky & Martin)

---

## ğŸ”§ ë¬¸ì œ í•´ê²°

### í”í•œ ì˜¤ë¥˜

**1. Out of Memory (OOM)**
```
ì›ì¸: Attention matrix O(nÂ²)
í•´ê²°:
  - Batch size ì¤„ì´ê¸°
  - Gradient checkpointing
  - Sequence length ì œí•œ
```

**2. NaN Loss**
```
ì›ì¸:
  - Learning rate ë„ˆë¬´ í¼
  - Gradient explosion
  - Numerical instability

í•´ê²°:
  - Learning rate warm-up
  - Gradient clipping
  - Mixed precision training
```

**3. Underfitting**
```
ì›ì¸: ëª¨ë¸ ìš©ëŸ‰ ë¶€ì¡±
í•´ê²°:
  - d_model ì¦ê°€
  - Layer ì¶”ê°€
  - d_ff ì¦ê°€
```

**4. Overfitting**
```
ì›ì¸: ëª¨ë¸ì´ ë„ˆë¬´ í¼ or ë°ì´í„° ë¶€ì¡±
í•´ê²°:
  - Dropout ì¦ê°€
  - ë°ì´í„° augmentation
  - ì •ê·œí™” ê°•í™”
```

### ì„±ëŠ¥ ìµœì í™”

**ë¹ ë¥¸ ì‹¤í—˜:**
```
- ì‘ì€ d_model (32-64)
- ì ì€ heads (2-4)
- ì§§ì€ sequence
- ì‘ì€ batch
```

**ìµœì¢… ëª¨ë¸:**
```
- í° d_model (256-512)
- ë§ì€ heads (8-16)
- ê¸´ sequence
- í° batch (GPU ë©”ëª¨ë¦¬ í—ˆìš© ë²”ìœ„)
```

---

## ğŸ“ í•™ìŠµ ì ê²€

### ê¸°ë³¸ ê°œë…

- [ ] Attentionì´ ì™œ í•„ìš”í•œì§€ ì„¤ëª…í•  ìˆ˜ ìˆë‚˜?
- [ ] Query, Key, Valueì˜ ì—­í• ì„ ì´í•´í•˜ë‚˜?
- [ ] Scaled Dot-Product Attention ì‹ì„ ì“¸ ìˆ˜ ìˆë‚˜?
- [ ] Self-Attentionê³¼ ì¼ë°˜ Attentionì˜ ì°¨ì´ëŠ”?

### ì¤‘ê¸‰ ê°œë…

- [ ] Multi-Head Attentionì´ ì™œ í•„ìš”í•œì§€ ì•„ëŠ”ê°€?
- [ ] Positional Encodingì´ ì—†ìœ¼ë©´ ì–´ë–»ê²Œ ë˜ë‚˜?
- [ ] Sinusoidal encodingì˜ ì¥ì ì€?
- [ ] Residual Connectionì˜ ì—­í• ì€?

### ê³ ê¸‰ ê°œë…

- [ ] Layer Normalization vs Batch Normalization?
- [ ] Pre-Norm vs Post-Norm ì°¨ì´ëŠ”?
- [ ] Transformerì˜ ë³µì¡ë„ O(nÂ²)ë¥¼ ì¤„ì´ëŠ” ë°©ë²•ì€?
- [ ] Attention weightsë¥¼ ì–´ë–»ê²Œ í•´ì„í•˜ë‚˜?

### ì‹¤ìŠµ ê³¼ì œ

1. **Attention ë¶„ì„:**
   - ë‹¤ì–‘í•œ ë¬¸ì¥ì—ì„œ attention pattern ê´€ì°°
   - ì–´ë–¤ ë‹¨ì–´ê°€ ì–´ë””ì— ì£¼ëª©í•˜ëŠ”ì§€ ë¶„ì„

2. **Position ì‹¤í—˜:**
   - Positional encoding ì—†ì´ í•™ìŠµ
   - ì„±ëŠ¥ ì°¨ì´ ì¸¡ì •

3. **Head ë¹„êµ:**
   - Head ìˆ˜ ë³€í™”ì‹œí‚¤ë©° ì„±ëŠ¥ ë¹„êµ
   - 1, 2, 4, 8 heads

4. **RNN ë¹„êµ:**
   - ë‹¤ì–‘í•œ ì‹œí€€ìŠ¤ ê¸¸ì´ì—ì„œ ì„±ëŠ¥ ë¹„êµ
   - ê³„ì‚° ì‹œê°„ ì¸¡ì •

---

## âœ¨ ê²°ë¡ 

**ë°°ìš´ ê²ƒ:**
- Attention: ë™ì ìœ¼ë¡œ ì¤‘ìš”í•œ ì •ë³´ ì„ íƒ
- Self-Attention: ì…ë ¥ ë‚´ë¶€ì˜ ê´€ê³„ í•™ìŠµ
- Multi-Head: ë‹¤ì–‘í•œ íŒ¨í„´ ë³‘ë ¬ í•™ìŠµ
- Positional Encoding: ìˆœì„œ ì •ë³´ ì¶”ê°€
- Transformer Block: ëª¨ë“  ìš”ì†Œì˜ ì¡°í™”

**í•µì‹¬ í†µì°°:**
```
"Attention Is All You Need"

ìˆœì°¨ ì²˜ë¦¬ (Sequential) â†’ ë³‘ë ¬ ì²˜ë¦¬ (Parallel)
ê³ ì • ë¬¸ë§¥ (Fixed) â†’ ë™ì  ë¬¸ë§¥ (Dynamic)
ë¶ˆíˆ¬ëª… (Opaque) â†’ í•´ì„ ê°€ëŠ¥ (Interpretable)
```

**ì˜í–¥:**
- NLP ì „ ë¶„ì•¼ì—ì„œ SOTA
- Computer Visionìœ¼ë¡œ í™•ì¥
- Multi-modal AIì˜ ê¸°ë°˜
- ChatGPT, GPT-4ì˜ í•µì‹¬
- AI ì—°êµ¬ì˜ ìƒˆë¡œìš´ íŒ¨ëŸ¬ë‹¤ì„

**ë‹¤ìŒ ë‹¨ê³„:**
- Pre-training (BERT, GPT ìŠ¤íƒ€ì¼)
- Fine-tuning ê¸°ë²•
- Prompt Engineering
- Vision Transformer (ViT)
- Efficient Transformers

---

*"Attentionì€ ë‹¨ìˆœí•œ ë©”ì»¤ë‹ˆì¦˜ì´ì§€ë§Œ, í˜„ëŒ€ AIì˜ í•µì‹¬ì…ë‹ˆë‹¤!"*

TransformerëŠ” Deep Learning ì—­ì‚¬ì˜ ì „í™˜ì ì…ë‹ˆë‹¤. ì´ ì£¼ì°¨ì—ì„œ ë°°ìš´ ë‚´ìš©ì€ ChatGPT, DALL-E, Stable Diffusion ë“± ìµœì‹  AI ì‹œìŠ¤í…œì„ ì´í•´í•˜ëŠ” ê¸°ì´ˆê°€ ë©ë‹ˆë‹¤!
